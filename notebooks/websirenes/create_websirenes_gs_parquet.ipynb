{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pandera as pa\n",
    "from pandera.typing import DataFrame\n",
    "from typing import Annotated\n",
    "import os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_websirenes = pd.read_parquet('websirenes_stations_original.parquet')\n",
    "df_websirenes.estacao = df_websirenes.estacao.str.strip()\n",
    "\n",
    "estacao_desc = df_websirenes['estacao_desc']\n",
    "estacao_desc = estacao_desc.str.strip().str.lower().str.replace(r'\\s+', '_', regex=True).replace(r'/', '.', regex=True)\n",
    "df_websirenes['estacao_desc'] = estacao_desc\n",
    "\n",
    "df_websirenes.to_parquet('websirenes_stations.parquet', index=False)\n",
    "df_websirenes.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_websirenes['estacao_desc'].replace('la', '.', regex=False).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_id = \"urca\"\n",
    "df = pd.read_parquet(\"../../data/ws/alertario/rain_gauge/\" + station_id + \".parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_timezones_name():\n",
    "    print(pytz.all_timezones)\n",
    "\n",
    "def get_UTC_offset_from_timezone_name(timezone_name: str) -> str:\n",
    "    return datetime.now(pytz.timezone(timezone_name)).strftime('%z')\n",
    "\n",
    "class WebSireneSchema(pa.DataFrameModel):\n",
    "    horaLeitura: pa.typing.Index[Annotated[\n",
    "        pd.DatetimeTZDtype, \"ns\", f\"UTC{get_UTC_offset_from_timezone_name('America/Sao_Paulo')}\"]\n",
    "    ]\n",
    "    nome: str\n",
    "    m15: float = pa.Field(nullable=True)\n",
    "    m30: float = pa.Field(nullable=True)\n",
    "    h01: float = pa.Field(nullable=True)\n",
    "    h02: float = pa.Field(nullable=True)\n",
    "    h03: float = pa.Field(nullable=True)\n",
    "    h04: float = pa.Field(nullable=True)\n",
    "    h24: float = pa.Field(nullable=True)\n",
    "    h96: float = pa.Field(nullable=True)\n",
    "    station_id: int\n",
    "\n",
    "class WebSirenesParser:\n",
    "    def list_files(self) -> list[str]:\n",
    "        return os.listdir('websirenes_defesa_civil')\n",
    "    \n",
    "    def _get_name_pattern(self) -> str:\n",
    "        return r'^(?P<name>.+?)(?=\\s+\\d{4}-\\d{2}-\\d{2})'\n",
    "    \n",
    "    def _get_date_pattern(self) -> str:\n",
    "        return r'(?P<date>\\d{4}-\\d{2}-\\d{2}\\s\\d{2}:\\d{2}:\\d{2}-\\d{2})'\n",
    "    \n",
    "    def _get_timeframe_pattern(self) -> str:\n",
    "        return r\"(?P<timeframe>.+)$\"\n",
    "    \n",
    "    def _get_complete_pattern(self) -> str:\n",
    "        return rf\"{self._get_name_pattern()}\\s+{self._get_date_pattern()}\\s+{self._get_timeframe_pattern()}\"\n",
    "\n",
    "    def _extract_features(self, line: str) -> tuple:\n",
    "        complete_pattern = self._get_complete_pattern()\n",
    "\n",
    "        match = re.search(complete_pattern, line)\n",
    "        \n",
    "        name = match.group('name')\n",
    "        date = match.group('date') + '00'\n",
    "        timeframe = match.group('timeframe')\n",
    "\n",
    "        m15, m30, h01, h02, h03, h04, h24, h96, station_id = [\n",
    "            np.nan if x == 'null' else float(x.replace(',', '.')) if \",\" in x else float(x)\n",
    "            for x in timeframe.strip().split()\n",
    "        ]\n",
    "\n",
    "        return (\n",
    "            name, \n",
    "            datetime.strptime(date, '%Y-%m-%d %H:%M:%S%z'), \n",
    "            m15, m30, h01, h02, h03, h04, h24, h96, \n",
    "            int(station_id)\n",
    "        )\n",
    "\n",
    "    def _parse_txt_file(self, file_path: str) -> tuple[list[str], list[tuple]]:\n",
    "        file_data: list[tuple] = []\n",
    "        with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "            header = file.readline().strip().split()\n",
    "            for line in file:\n",
    "                file_data.append(self._extract_features(line))\n",
    "        return header, file_data\n",
    "    \n",
    "    def read_station_name_id_txt_file(self, file_path: str) -> tuple[str, int]:\n",
    "        with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "            header = file.readline().strip().split()\n",
    "            nome, horaLeitura, m15, m30, h01, h02, h03, h04, h24, h96, station_id = self._extract_features(\n",
    "                file.readline()\n",
    "            )\n",
    "            return nome, station_id\n",
    "        \n",
    "    def get_time_resolution(self, dates: pd.Series)-> dict:\n",
    "        return dict(Counter([dates[i] - dates[i - 1] for i in range(1, len(dates))]))\n",
    "    \n",
    "    def get_dataframe(self, file_path: str) -> DataFrame[WebSireneSchema]:\n",
    "        header, file_data = self._parse_txt_file(file_path)\n",
    "        df = pd.DataFrame(file_data, columns=header)\n",
    "        df.rename(columns={'id': 'station_id'}, inplace=True)\n",
    "        df.set_index('horaLeitura', inplace=True)\n",
    "        validated_df = WebSireneSchema.validate(df)\n",
    "        return df\n",
    "    \n",
    "    def assert_is_sorted_by_date(self, df: DataFrame[WebSireneSchema]) -> bool:\n",
    "        assert df.index.is_monotonic_increasing, 'DataFrame index is not sorted by date'\n",
    "\n",
    "websirenes_parser = WebSirenesParser()\n",
    "\n",
    "for file in websirenes_parser.list_files():\n",
    "    df = websirenes_parser.get_dataframe(os.path.join('websirenes_defesa_civil', file))\n",
    "    websirenes_parser.assert_is_sorted_by_date(df)\n",
    "    print(f'Data de início de operação: {df.index[0]}')\n",
    "    break\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['m15'].isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'names': ['my name', 'my     name', '     my  name 2   / name 3 ']\n",
    "})\n",
    "\n",
    "names = df['names']\n",
    "cleaned_names = names.str.strip().str.lower().str.replace(r'\\s+', '_', regex=True).replace(r'/', '.', regex=True)\n",
    "\n",
    "print(cleaned_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    return ''.join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "\n",
    "df_websirenes_estacao_desc_unique = df_websirenes['estacao_desc'].unique()\n",
    "\n",
    "def produce_hourly_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    estacao_desc = df['nome']\n",
    "    estacao_desc = estacao_desc.str.strip()\n",
    "    estacao_desc = estacao_desc.str.lower()\n",
    "    estacao_desc = estacao_desc.apply(remove_accents)\n",
    "    estacao_desc = estacao_desc.str.replace(r'\\s+', '_', regex=True)\n",
    "    estacao_desc = estacao_desc.replace(r'/', '.', regex=True)\n",
    "    \n",
    "    df['estacao_desc'] = estacao_desc\n",
    "    stacao_desc_name = df['estacao_desc'].iloc[0]\n",
    "\n",
    "    if stacao_desc_name not in df_websirenes_estacao_desc_unique:\n",
    "        error_color = '\\033[91m'\n",
    "        reset_color = '\\033[0m'\n",
    "        print(f\"{error_color}Station {stacao_desc_name} not found in websirenes_stations.parquet{reset_color}\")\n",
    "        return None\n",
    "\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={\n",
    "        'horaLeitura': 'datetime',\n",
    "        'm15': 'precipitation_sum'\n",
    "    }, inplace=True)\n",
    "\n",
    "    hourly_df = df.loc[:, ['datetime', 'estacao_desc', 'precipitation_sum']]\n",
    "    hourly_df['datetime'] = pd.to_datetime(hourly_df['datetime'])\n",
    "    hourly_df['precipitation_sum'] = hourly_df['precipitation_sum'].ffill(limit_area=\"inside\", limit=4)\n",
    "    hourly_df['precipitation_sum'] = hourly_df['precipitation_sum'].bfill(limit_area=\"inside\", limit=4)\n",
    "    hourly_df = hourly_df[hourly_df.datetime.dt.minute == 0]\n",
    "\n",
    "    if hourly_df['precipitation_sum'].isnull().values.any().any():\n",
    "        print(f\"Sirene Station {stacao_desc_name} has missing precipitation {(hourly_df.isnull().mean() * 100).mean()}\")\n",
    "    return hourly_df\n",
    "\n",
    "for i, file in enumerate(websirenes_parser.list_files()):\n",
    "    print(f\"Processing {i}/{len(websirenes_parser.list_files())}\")\n",
    "    df = websirenes_parser.get_dataframe(os.path.join('websirenes_defesa_civil', file))\n",
    "    websirenes_parser.assert_is_sorted_by_date(df)\n",
    "    df = produce_hourly_data(df)\n",
    "    if df is None: continue\n",
    "    print(f'Data de início de operação: {df.datetime.min()}')\n",
    "    station_name = df['estacao_desc'].iloc[0]\n",
    "    df.to_parquet(f\"{station_name}.parquet\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atmoseer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
